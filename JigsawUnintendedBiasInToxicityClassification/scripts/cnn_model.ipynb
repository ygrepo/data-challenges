{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os libraries\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "# Analytic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "MAX_NUM_WORDS = 10000\n",
    "TOXICITY_COLUMN = 'target'\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "\n",
    "EMBEDDINGS_PATH = '../input/Glove_6B/glove.6B.100d.txt'\n",
    "EMBEDDINGS_DIMENSION = 100\n",
    "DROPOUT_RATE = 0.3\n",
    "LEARNING_RATE = 0.00005\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "MAX_NUM_WORDS = 10000\n",
    "TOXICITY_COLUMN = 'target'\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "MODEL_NAME = 'my_model'\n",
    "\n",
    "# All comments must be truncated or padded to be the same length.\n",
    "MAX_SEQUENCE_LENGTH = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jigsaw-unintended-bias-in-toxicity-classification', '.DS_Store', 'glove.840B.300d.pkl', 'Glove_6B', 'crawl-300d-2M.pkl']\n",
      "['glove.6B.100d.txt', 'glove.6B.100d.txt.zip']\n",
      "['small_train.csv', 'test.csv', 'train.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"../input\"))\n",
    "print(os.listdir(\"../input/Glove_6B\"))\n",
    "print(os.listdir(\"../input/jigsaw-unintended-bias-in-toxicity-classification\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all identities\n",
    "IDENTITY_COLUMNS = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target and identity columns to booleans\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "\n",
    "\n",
    "def convert_dataframe_to_bool(df):\n",
    "    bool_df = df.copy()\n",
    "    for col in ['target'] + IDENTITY_COLUMNS:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df\n",
    "\n",
    "# Create a text tokenizer.\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "def pad_text(texts):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "def get_embeddings():\n",
    "    # Load embeddings\n",
    "    print('loading embeddings')\n",
    "    embeddings_index = {}\n",
    "    with open(EMBEDDINGS_PATH) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n",
    "                                 EMBEDDINGS_DIMENSION))\n",
    "    num_words_in_embedding = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            num_words_in_embedding += 1\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        # else:\n",
    "        #     print(\"Word not found in embeddings:\" + word)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(123)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        embedding_matrix = get_embeddings()\n",
    "        embedding_tensor = torch.from_numpy(embedding_matrix).double()\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
    "        self.conv1 = nn.Conv1d(in_channels=100, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=4, stride=1, padding=2)\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.embedding_layer(x.long())\n",
    "        x = x.view(x.shape[0], x.shape[2], x.shape[1])\n",
    "        x = self.conv1(x.float())\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 40)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.dropout(x, DROPOUT_RATE)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.squeeze(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, model, optimizer, train_loader, criterion):\n",
    "    model.train()  # Prepare data\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def convert_value_to_binary(v):\n",
    "    return 1. if v > 0.5 else 0.\n",
    "\n",
    "\n",
    "def convert_array_to_binary(a):\n",
    "    return np.array([convert_value_to_binary(v) for v in a])\n",
    "\n",
    "\n",
    "def compare_tensors(a, b):\n",
    "    a = a.cpu()\n",
    "    b = b.cpu()\n",
    "    correct = 0\n",
    "    for x, y in zip(a, b):\n",
    "        if torch.equal(x.float(), y.float()):\n",
    "            correct += 1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def predict(data, model):\n",
    "    data = data.to(device)\n",
    "    output = model(data)\n",
    "    return np.apply_along_axis(convert_array_to_binary, 0, output.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "def test(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    prediction_list = []\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = np.apply_along_axis(convert_array_to_binary, 0, output.cpu().detach().numpy())\n",
    "        prediction_list.append([x.tolist() for x in pred])\n",
    "        pred = torch.from_numpy(pred)\n",
    "        correct += compare_tensors(pred, target)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "          .format(test_loss, correct, len(test_loader.dataset), accuracy))\n",
    "\n",
    "\n",
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np = 0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(device, train_df):\n",
    "    print('%d initial training data' % len(train_df))\n",
    "\n",
    "    train_data_df, validate_df = model_selection.train_test_split(train_df, test_size=0.1)\n",
    "    print('%d train comments, %d validate comments' % (len(train_data_df), len(validate_df)))\n",
    "    tokenizer.fit_on_texts(train_data_df[TEXT_COLUMN])\n",
    "\n",
    "    train_text = pad_text(train_data_df[TEXT_COLUMN])\n",
    "    train_labels = np.array(train_data_df[TOXICITY_COLUMN], dtype='int')\n",
    "    train_tensor = torch_data.TensorDataset(torch.FloatTensor(train_text), torch.FloatTensor(train_labels))\n",
    "    train_loader = torch_data.DataLoader(train_tensor, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    validate_text = pad_text(validate_df[TEXT_COLUMN])\n",
    "    validate_labels = np.array(validate_df[TOXICITY_COLUMN], dtype='int')\n",
    "    validate_tensor = torch_data.TensorDataset(torch.FloatTensor(validate_text), torch.FloatTensor(validate_labels))\n",
    "    validate_loader = torch_data.DataLoader(validate_tensor, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    model = CNN().float()\n",
    "    model.to(device)\n",
    "    print('Number of parameters: {}'.format(get_n_params(model)))\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=LEARNING_RATE)\n",
    "    for epoch in range(0, NUM_EPOCHS):\n",
    "        train_model(epoch, model, optimizer, train_loader, criterion)\n",
    "        test(model, criterion, validate_loader)\n",
    "\n",
    "    validate_df.loc[:, MODEL_NAME] = predict(torch.FloatTensor(validate_text), model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(device, train_df):\n",
    "    try:\n",
    "        model = train_epoch(device, train_df)\n",
    "        test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "        test = test.head(100)\n",
    "        test_text = pad_text(test[TEXT_COLUMN])\n",
    "        submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv',\n",
    "                             index_col='id')\n",
    "        submission = submission.head(100)\n",
    "        submission['prediction'] = predict(torch.FloatTensor(test_text), model)\n",
    "        submission.to_csv('submission.csv')\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 100 records\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "train = train.head(100)\n",
    "print('loaded %d records' % len(train))\n",
    "\n",
    "# Make sure all comment_text values are strings\n",
    "train['comment_text'] = train['comment_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = convert_dataframe_to_bool(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 initial training data\n",
      "100 train comments, 10 validate comments\n",
      "loading embeddings\n",
      "Number of parameters: 313913\n",
      "Train Epoch: 0 [0/90 (0%)]\tLoss: 0.695042\n",
      "\n",
      "Test set: Average loss: 0.0673, Accuracy: 10/10 (100%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "submit(device, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
